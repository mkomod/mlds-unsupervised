---
title: "Tutorial: EM alogrithm"
output:
  html_document:
    css: ./style.css
---

```{r, echo=FALSE}
library(mvtnorm)
```

---

## Mixture Models

Mixture models comprise of a linear combination of other distributions. For example, consider the distribution,
$$
Z = 0.3 N(\mu_1, \Sigma_1) + 0.2N(\mu_2, \Sigma_2) + 0.5N(\mu_3, \Sigma_3)
$$
where $\mu_k \in \mathbb{R}^d$ is the mean vector and $\Sigma_k \in \mathbb{R}^{d \times d}$ is the covariance matrix for $k=1,2,3$. We notice that $Z$ comprises of a mixture of Gaussians, where each Gaussian, referred to as a **component** of the mixture, has its own mean $\mu_k$ and covariance $\Sigma_k$.

$Z$ is commonly referred to as a **Gaussian mixture model**, whose density is more generally written as,
$$
p(x | \theta) = \sum_{k=1}^K \pi_k \phi(x; \mu_k, \Sigma_k)
$$
where $K \in \mathbb{N}$ is the number of components, $\pi_k \in [0, 1]$ are the **mixing coefficients** such that $\sum_{k=1}^K = 1$, and $\phi(x; \mu_k, \Sigma_k)$ is the pdf of multivariate Normal distribution with mean $\mu_k$ and covariance $\Sigma_k$. For notational shorthand we write the model parameters as $\theta = \{ \pi_k, \mu_k, \Sigma_k \}_{k=1}^K$.

We'll see later on that GMMs are a useful probabilistic tool that are able to capture detailed structure in our data. For the meantime, we conclude by noting that mixture models are not limited to continuous distributions and have been extended to discrete distributions such as Bernoulli mixtures.

--- 

### Exercise {.tabset}

Implement a function to sample from a Gaussian mixture model. A description of the function arguments are given below.

#### Template

```{r, echo}
library(mvtnorm)  # used to sample from a multivariate Gaussian

sample.gmm <- function(n, p, mu, Sig)
{
    # n    the number of samples
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns a (n, d) matrix of the samples


}
```



#### Solution

```{r}
library(mvtnorm)  # used to sample from a multivariate Gaussian

sample.gmm <- function(n, p, mu, Sig)
{
    # n    the number of samples
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns a (n, d) matrix of the samples
    if (sum(p) != 1) stop("p must sum to 1")

    d <- ncol(mu) 
    K <- nrow(mu)
    res <- matrix(NA, nrow=n, ncol=d)  # store the samples

    for (i in 1:n) {
        k <- sample(1:K, size=1, prob=p)      # component to sample from
        res[i, ] <- rmvnorm(1, mean=mu[k, ], sigma=as.matrix(Sig[, , k]))
    }

    return(res)
}
```

####  Testing

Test your function with the following test cases:

```{r}
# test #1
p <- c(0.3, 0.2, 0.5)
mu <- matrix(c(-3, 0, 4), ncol=1)
Sig <- array(c(0.5, 1, 0.4), dim=c(1, 1, 3))

X <- sample.gmm(100, p, mu, Sig)


# test #2
p <- c(0.3, 0.2, 0.5)
mu <- matrix(c(-3, 5, 5, 3, 3, -4), ncol=2, byrow=TRUE)
Sig <- array(c(1, 0, 0, 1,
	           0.2, 0.1, 0.1, 0.2,
	           1.5, 0.3, 0.3, 0.3), dim=c(2, 2, 3))

X <- sample.gmm(100, p, mu, Sig)
```


### {-}

---

Let's take a look at the samples form the second test.

```{r, fig.align="center", fig.height=6, fig.width=6}
X <- sample.gmm(100, p, mu, Sig)

plot(X, pch=20, xlim=c(-8, 8), ylim=c(-8, 8), 
     xlab=expression(X[1]), ylab=expression(X[2]))
```

We notice that there are three distinct groups, with each group containing a different proportion of samples.


---

## The EM alogrithm

Recall, the EM alogrithm is used to find maximum likelihood solutions for models with latent variables. The EM algorithm has been covered during the lectures, however, as we'll be referring to the algorithm throughout the tutorial it is given below for reference: 

 1. Initialize $\theta$, the model parameters
 2. E-step: Evaluate $p(Z | X, \theta^{\text{old}})$
 3. M-step: Compute $\theta^{\text{new}}$ given by solving,
    $$
    \theta^\text{new} = \underset{\theta}{\arg\! \max}\ Q(\theta, \theta^\text{old})
    $$
    where
    $$
    Q(\theta, \theta^\text{old}) = \sum_{Z} p(Z | X, \theta^\text{old}) \log p(X, Z | \theta)
    $$
 4. Check for convergence. If reached terminate, else set $\theta^\text{old} \leftarrow \theta^\text{new}$

---

## The EM algorithm for Gaussian Mixture Models

We now turn to implementing the EM algorithm for Gaussian mixture models. At this point you may be wondering "Where are the latent variables for the GMM?".

This isn't immediately obvious, so to highlight that there are in fact latent variables, let's consider the following question: how would you alter the function `sample.gmm` so that we know which group the sample came from?

We see that, to know which group each sample came from we would have to return `k` as well. This is because `k` tells us which component to draw the sample from. As the group label is unknown to us, it in turn, is an unknown latent variable which we want to recover.

---

### Formulating the problem

Now that we've established that there are latent variables, let's formulate the problem more precisely. We begin by defining the full dataset and the corresponding probability densities needed to implement the EM alogrithm.

Let our full dataset of $i=1,\dots,n$ samples be given by $\{ X, Z \} = \{ (x_i)_{i=1}^n, (z_i)_{i=1}^n \}$ where $x_i \in \mathbb{R}^{1 \times d}$ is a realization from a Gaussian component, and $z_i$ is a $d$-dimensional vector with $0$s everywhere except for the index of the component, for which it takes value $1$. 

For example, if $x_i$ came from the second component then $z_i = (0, 1, 0)$.

---

### Implementation

To implement the EM algorithm we need to know two things:

 1. The posterior $p(z | x, \theta)$.
 2. The joint likelihood $p(x, z | \theta)$.

Currently, we know the form of $p(x | \theta)$ as well as $p(z | \theta) = \prod_{k=1}^K \pi_k^{z_k}$ where $z_k$ denotes the $k$th element of $z$. 

We can also deduce the form of $p(x | z, \theta)$, which is the density of $x$ given we know which component it came from, i.e.
$$
p(x | \theta, z) = \prod_{k=1}^K \phi(x; \mu_k, \Sigma_k)^{z_k}
$$


---

### Exercise {.tabset}

Write down the mass function of $p(z | x, \theta)$ up to a normalizing constant,

#### Template

**hint:** use Bayes theorem

$$
p(z | x, \theta)  \propto
$$


#### Solution


$$
p(z | x, \theta) \propto p(z | \theta) p(x | z, \theta)
	= \prod_{k=1}^K \pi_k^{z_k} \phi(x; \mu_k, \Sigma_k)^{z_k}
$$


### {-}

---

Now we define

$$
\gamma(z_k) = p(z_k = 1 | x, \theta) = \frac{\pi_k \phi(x | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_k \phi(x; \mu_k, \Sigma_k)}
$$


### Exercise {.tabset}

Implement a function for $\gamma(z_k)$

#### Template

```{r}
g <- function(k, x, p, mu, Sig) 
{
    # k    the component index
    # x    the value of x
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns probability 

}
```

#### Solution

```{r}
g <- function(k, x, p, mu, Sig) 
{
    # k    the component index
    # x    the value of x
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns probability 
    K <- nrow(mu)
    
    num <- p[k] * dmvnorm(x, mean=mu[k, ], sigma=as.matrix(Sig[ , , k]))
    den <- sum(sapply(1:K, function(j) p[j] * dmvnorm(x, mean=mu[j, ], sigma=as.matrix(Sig[ , , j]))))
    
    return(num / den)
}
```

#### Testing

```{r}
g(1, X[1, ], p, mu, Sig)
```

### {-}

--- 

Finally, we must derive the joint log-likelihood $p(x, z | \theta)$. In a similar fashion as before, we have,

$$
p(z, x | \theta) = p(x | z, \theta) p (z | \theta) 
		 = \prod_{k=1}^K \pi^{z_k}_k \phi(x; \mu_k, \Sigma_k)^{z_k}
$$

Therefore the likelihood over the full dataset is given by

$$
p(X, Z | \theta) = \prod_{i=1}^n \prod_{k=1}^K \pi^{z_{ik}}_k \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}}
$$

and the corresponding log-likelihood by

$$
\log p(X, Z | \theta) = \sum_{i=1}^n \sum_{k=1}^K \log \left( \pi^{z_{ik}}_k \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}} \right)
$$

---

We are now able to write down a full expression for $Q(\theta, \theta^{\text{old}})$. The last task is to solve the optimization problem given by
$$
\theta^{\text{new}} = \underset{\theta}{ \arg \! \max}\  \sum_{Z}  p(Z | X, \theta) \log p(X, Z | \theta)
$$
There are two approaches we can take to solve this problem, the first is using optimization routines and the second is via an analytic solution. 

Given it is possible to derive an analytic solution in this case, we pursue the later.


