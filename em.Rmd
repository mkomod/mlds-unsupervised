---
title: "Tutorial: EM alogrithm"
output:
  html_document:
    css: ./style.css
---

```{r, echo=FALSE}
library(mvtnorm)
```

---

## Mixture Models

Mixture models are made up of a linear combination of other distributions. We'll see later on that mixture models are a useful probabilistic tool that are able to capture detailed structure in our data. 

In the meantime, consider let's consider the distribution,
$$
Z = 0.3 N(\mu_1, \Sigma_1) + 0.2N(\mu_2, \Sigma_2) + 0.5N(\mu_3, \Sigma_3)
$$
where $\mu_k \in \mathbb{R}^d$ is the mean vector and $\Sigma_k \in \mathbb{R}^{d \times d}$ is the covariance matrix for $k=1,2,3$. We notice that $Z$ comprises of a mixture of Gaussians, where each Gaussian, referred to as a **component** of the mixture, has its own mean $\mu_k$ and covariance $\Sigma_k$. 

Such distributions are commonly referred to as **Gaussian mixture models**, whose density is written as,
$$
p(x | \theta) = \sum_{k=1}^K \pi_k \phi(x; \mu_k, \Sigma_k)
$$
where $K \in \mathbb{N}$ is the number of components, $\pi_k \in [0, 1]$ are the **mixing coefficients** such that $\sum_{k=1}^K = 1$, and $\phi(x; \mu_k, \Sigma_k)$ is the pdf of multivariate Normal distribution with mean $\mu_k$ and covariance $\Sigma_k$. For notational shorthand we typically write the model parameters as $\theta = \{ \pi_k, \mu_k, \Sigma_k \}_{k=1}^K$.

**Aim:** our aim in this tutorial is to construct an EM algorithm for the Gaussian mixture model.

--- 

## The EM alogrithm

The EM algorithm has been covered during the lectures, however, as we'll be referring to the algorithm throughout the tutorial it is given below for reference.

Recall, the EM algorithm is used to find maximum likelihood solutions for models with latent variables. 

 1. Initialize $\theta$, the model parameters
 2. E-step: Evaluate $p(Z | X, \theta^{\text{old}})$
 3. M-step: Compute $\theta^{\text{new}}$ given by solving,
    $$
    \theta^\text{new} = \underset{\theta}{\arg\! \max}\ Q(\theta, \theta^\text{old})
    $$
    where
    $$
    Q(\theta, \theta^\text{old}) = \sum_{Z} p(Z | X, \theta^\text{old}) \log p(X, Z | \theta)
    $$
 4. Check for convergence. If reached terminate, else set $\theta^\text{old} \leftarrow \theta^\text{new}$

---


### Exercise {.tabset}

Implement a function to sample from a Gaussian mixture model. A description of the function arguments are given below.

#### Template

```{r, echo}
library(mvtnorm)  # used to sample from a multivariate Gaussian

sample.gmm <- function(n, p, mu, Sig)
{
    # n    the number of samples
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns a (n, d) matrix of the samples


}
```



#### Solution

```{r}
library(mvtnorm)  # used to sample from a multivariate Gaussian

sample.gmm <- function(n, p, mu, Sig)
{
    # n    the number of samples
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns a (n, d) matrix of the samples
    if (sum(p) != 1) stop("p must sum to 1")

    d <- ncol(mu) 
    K <- nrow(mu)
    res <- matrix(NA, nrow=n, ncol=d)  # store the samples

    for (i in 1:n) {
        k <- sample(1:K, size=1, prob=p)      # component to sample from
        res[i, ] <- rmvnorm(1, mean=mu[k, ], sigma=as.matrix(Sig[, , k]))
    }

    return(res)
}
```

####  Testing

Test your function with the following test cases:

```{r}
# test #1
p <- c(0.3, 0.2, 0.5)
mu <- matrix(c(-3, 0, 4), ncol=1)
Sig <- array(c(0.5, 1, 0.4), dim=c(1, 1, 3))

X <- sample.gmm(100, p, mu, Sig)


# test #2
p <- c(0.3, 0.2, 0.5)
mu <- matrix(c(-3, 5, 5, 3, 3, -4), ncol=2, byrow=TRUE)
Sig <- array(c(1, 0, 0, 1,
	           0.2, 0.1, 0.1, 0.2,
	           1.5, 0.3, 0.3, 0.3), dim=c(2, 2, 3))

X <- sample.gmm(100, p, mu, Sig)
```


### {-}

---

Let's take a look at the samples form the second test.

```{r, fig.align="center", fig.height=6, fig.width=6}
X <- sample.gmm(100, p, mu, Sig)

plot(X, pch=20, xlim=c(-8, 8), ylim=c(-8, 8), 
     xlab=expression(X[1]), ylab=expression(X[2]))
```

We notice that there are three distinct groups, with each group containing a different proportion of samples.


---

## The EM algorithm for Gaussian Mixture Models

We now turn to implementing the EM algorithm for Gaussian mixture models. At this point you may be wondering "Where are the latent variables for the GMM?".

This isn't immediately obvious, so to highlight that there are in fact latent variables, let's consider the following question: how would you alter the function `sample.gmm` so that we know which group the sample came from?

We see that, to know which group each sample came from we would have to return `k` as well. This is because `k` tells us which component to draw the sample from. As the group label is unknown to us, it in turn, is an unknown latent variable which we want to recover.

---

### Formulating the problem

Now that we've established that there are latent variables, let's formulate the problem more precisely. We begin by defining the full dataset and the corresponding probability densities needed to implement the EM algorithm.

Let our full dataset of $i=1,\dots,n$ samples be given by $\{ X, Z \} = \{ (x_i)_{i=1}^n, (z_i)_{i=1}^n \}$ where $x_i \in \mathbb{R}^{1 \times d}$ is a realization from a Gaussian component, and $z_i$ is a $d$-dimensional vector with $0$s everywhere except for the index of the component, for which it takes value $1$. 

For example, if $x_i$ came from the second component then $z_i = (0, 1, 0)$.

---

### Implementation

To implement the EM algorithm we need to know two things:

 1. The posterior $p(z | x, \theta)$.
 2. The joint likelihood $p(x, z | \theta)$.

Currently, we know the form of $p(x | \theta)$ as well as $p(z | \theta) = \prod_{k=1}^K \pi_k^{z_k}$ where $z_k$ denotes the $k$th element of $z$. 

We can also deduce the form of $p(x | z, \theta)$, which is the density of $x$ given we know which component it came from, i.e.
$$
p(x | \theta, z) = \prod_{k=1}^K \phi(x; \mu_k, \Sigma_k)^{z_k}
$$


---

### Exercise {.tabset}

Write down the mass function of $p(z | x, \theta)$ up to a normalizing constant,

#### Template

**hint:** use Bayes theorem

$$
p(z | x, \theta)  \propto
$$


#### Solution


$$
p(z | x, \theta) \propto p(z | \theta) p(x | z, \theta)
	= \prod_{k=1}^K \pi_k^{z_k} \phi(x; \mu_k, \Sigma_k)^{z_k}
$$


### {-}

---

Now we define

$$
\gamma(z_k) = p(z_k = 1 | x, \theta) = \frac{\pi_k \phi(x | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_k \phi(x; \mu_k, \Sigma_k)}
$$


### Exercise {.tabset}

Implement a function that returns the vector
$$
(\pi_1 \phi(x_i | \mu_1, \Sigma_1), \dots, \pi_K \phi(x_i | \mu_K, \Sigma_K))
$$
for a given $x_i$.

#### Template

```{r}
g <- function(x, p, mu, Sig) 
{
    # x    the value of x
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns probability 

}
```

#### Solution

```{r}
g <- function(x, p, mu, Sig) 
{
    # x    the value of x
    # p    a K dimensional vector of mixing coefficients such that sum(p) = 1
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    #
    # returns probability 
    K <- nrow(mu)
    
    res <- sapply(1:K, function(j) p[j] * dmvnorm(x, mean=mu[j, ], sigma=as.matrix(Sig[ , , j])))
    return(res)    
}
```

#### Testing

```{r}
g(X[1, ], p, mu, Sig)
```

### {-}

--- 

Finally, we must derive the joint log-likelihood $p(x, z | \theta)$. In a similar fashion as before, we have,

$$
p(z, x | \theta) = p(x | z, \theta) p (z | \theta) 
		 = \prod_{k=1}^K \pi^{z_k}_k \phi(x; \mu_k, \Sigma_k)^{z_k}
$$

Therefore the likelihood over the full dataset is given by

$$
p(X, Z | \theta) = \prod_{i=1}^n \prod_{k=1}^K \pi^{z_{ik}}_k \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}}
$$

and the corresponding log-likelihood by

$$
\log p(X, Z | \theta) = \sum_{i=1}^n \sum_{k=1}^K \log \left( \pi^{z_{ik}}_k \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}} \right)
$$

---

We are now able to write down a full expression for $Q(\theta, \theta^{\text{old}})$. The last task is to solve the optimization problem given by
$$
\theta^{\text{new}} = \underset{\theta}{ \arg \! \max}\  \sum_{Z}  p(Z | X, \theta^{\text{old}}) \log p(X, Z | \theta)
$$
There are two approaches we can take to solve this problem, the first is using optimization routines and the second is via an analytic solution. 

Given it is possible to derive an analytic solution in this case, we pursue the later. We demonstrate the process for $\mu_k$, beginning by writing down the objective function more clearly,
$$
\begin{align}
f =&\ \sum_{Z}  p(Z | X, \theta^{\text{old}}) \log p(X, Z | \theta) \\
=&\ \sum_{i=1}^n \sum_{k=1}^K \gamma(z_{ik}) \log \left( \pi^{z_{ik}}_k \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}} \right) \\
=&\ \sum_{i=1}^n \sum_{k=1}^K \gamma(z_{ik}) \log \left( \pi^{z_{ik}}_k \right) + \gamma(z_{ik}) \log \left( \phi(x_i; \mu_k, \Sigma_k)^{z_{ik}} \right)
\end{align}
$$
Finding the optimum of $f$ with respect to $\mu_k$ now involves differentiating with respect to $\mu_k$, setting the expression to zero and rearranging. Formally, the derivative of $f$ with respect to $\mu_k$ is,


### Exercise {.tabset}

#### Template

Differentiate $f$ with respect to $\mu_k$, set to zero and re-arrange to find the optimizer of $f$,

$$
\frac{\partial f}{\partial \mu_k} =
$$

---

#### Solution

Differentiate $f$ with respect to $\mu_k$

$$
\frac{\partial f}{\partial \mu_k} = \sum_{i=1}^n \gamma(z_{ik}) \Sigma_{k}^{-1} (x_i - \mu_k).
$$
Setting to zero and re-arranging gives
$$
\widehat{\mu}_k = \frac{1}{N_k} \sum_{i=1}^n \gamma(z_{ik}) x_i
$$
where $N_k = \sum_{i=1}^n \gamma(z_{ik})$. We notice that this is very similar to the maximum likelihood estimator. 

---

### {-}


A similar approach can be taken to derive analytic updates for $\pi_k$ and $\Sigma_k$, however, for brevity we present these expressions below,
$$
\widehat{\Sigma}_k = \frac{1}{N_k} \sum_{i=1}^n \gamma(z_{ik}) (x_i - \mu_k)(x_i - \mu_k)^\top
$$
and
$$
\widehat{\pi}_k = \frac{N_k}{N}
$$

---

Great, now we can finish up our implementation. To simplify the computation, we are going to pre-compute $\gamma(z_{ik})$ for each $i$ and $k$ before updating the model parameters. For this we use the function `g` which we wrote earlier, passing each row of $X$ to `g` and returning the matrix `G`,

```{r}
G <- t(apply(X, 1, function(x) g(x, p, mu, Sig)))
G <- t(apply(G, 1, function(g) g / sum(g)))
```


### Exercise {.tabset}

Write a function to update $\mu$, this will involve updating each $\mu_k$.

#### Template

```{r}
update_mu <- function(mu, X, G) 
{ 
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # X    the (n, d) data matrix X
    # G    the (n, K) matrix of group weights
    # 
    # returns updated (K, d) matrix mu
    K <- nrow(mu)

    for (k in 1:K) 
    {
        # update mu_k
        # ...
    }
    return(mu)
} 
```

#### Solution

```{r}
update_mu <- function(mu, X, G) 
{ 
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    # X    the (n, d) data matrix X
    # G    the (n, K) matrix of group weights
    # 
    # returns updated (K, d) matrix mu
    K <- nrow(mu)

    for (k in 1:K) 
    {
        N_k <- sum(G[ , k])
        mu[k, ] <- 1/N_k * apply(G[ , k] * X, 2, sum)
    }
    return(mu)
} 
```

### {-}


### Exercise {.tabset}

Write a function to update the array of covariance matrices `Sig`, this will involve updating each $\Sigma_k$.

#### Template

```{r}
update_Sig <- function(Sig, X, G, mu) 
{ 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    # X	   the (n, d) data matrix
    # G    the (n, K) matrix of group weights
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    #
    # returns updated (d, d, K) array of covariance matrices, Sig.

    K <- nrow(mu)
    d <- ncol(mu)
    n <- nrow(X)

    for (k in 1:K)
    {
        # update Sig[ , , k]
        # ...
    }
    return(Sig)
} 
```

#### Solutions


```{r}
update_Sig <- function(Sig, X, G, mu) 
{ 
    # Sig  an array of dimension (d, d, K) where each entry corresponds to the
    #	   d x d covariance matrix 
    # X	   the (n, d) data matrix
    # G    the (n, K) matrix of group weights
    # mu   a (K, d) matrix where each row corresponds to the components mean 
    #	   vector of dimension d 
    #
    # returns updated (d, d, K) array of covariance matrices, Sig.
    K <- nrow(mu)
    p <- ncol(mu)
    N <- nrow(X)

    for (k in 1:K)
    {
        S <- matrix(0, nrow=p, ncol=p)
        for (n in 1:N) {
            # update S
            # ...
	    S <- S + G[n, k] * (X[n, ] - mu[k, ]) %*% t(X[n, ] - mu[k, ])
	}

	N_k <- sum(G[ , k])
	Sig[ , , k] <- S / N_k
    }
    return(Sig)
} 
```

### {-}

---

### Exercise {.tabset}

Write a function to update the vector of mixing coefficients `p`.

#### Template

```{r}
update_p <- function(p, G)
{ 
    # G    the (n, K) matrix of group weights
    #
    # returns a K dimensional vector of updated mixing coefficients.
    

} 
```

#### Solutions


```{r}
update_p <- function(G)
{ 
    # G    the (n, K) matrix of group weights
    #
    # returns a K dimensional vector of updated mixing coefficients.

    p <- apply(G, 2, sum) / sum(G)

    return(p)
} 
```

### {-}

---

## Piecing it all together.

First we save the true values, we'll compare the EM solutions to these later.

```{r}
mu.true <- mu
Sig.true <- Sig
p.true <- p
```

Now we write up the EM algorithm

```{r}
for (iter in 1:100) 
{
    # save the old values of mu, Sig, p
    mu.old <- mu
    Sig.old <- Sig
    p.old <- p

    # compute gamma(z_{ik})
    G <- t(apply(X, 1, function(x) g(x, p, mu, Sig)))
    G <- t(apply(G, 1, function(g) g / sum(g)))

    # update mu, Sig, p
    mu <- update_mu(mu, X, G)
    Sig <- update_Sig(Sig, X, G, mu)
    p <- update_p(G)
    
    # check for convergence
    if (sum(abs(mu.old - mu)) < 1e-3 &&
        sum(abs(Sig.old - Sig)) < 1e-3 &&
        sum(abs(p.old - p)) < 1e-3)
        break
}
```

Comparing the EM solution to the true values, we see the algorithm has done a great job at estimating the unknown parameter values under hidden latent variables. For instance, consider `p`,

```{r}
p
p.true
```

Visually, we can compare the density of the estimates to the observed data,

```{r, fig.width=9, fig.height=6, fig.align="center"}
compute.density <- function(x, mu, Sig, p) 
{
    K <- nrow(mu)
    res <- 0
    for (k in 1:K) {
        res <- res + dmvnorm(x, mean=mu[k, ], sigma=as.matrix(Sig[, , k]))
    }
    return(res)
}

xs <- seq(-10, 10, length.out=100)
z <- outer(xs, xs, Vectorize(function(x, y) compute.density(c(x, y), mu, Sig, p)))
filled.contour(xs, xs, z, plot.title={points(X, pch=20)}, 
	      color.palette=function(n) hcl.colors(n, palette="viridis"),
	      xlab=expression(X[1]), ylab=expression(X[2]))
```

